# -*- coding: utf-8 -*-
"""skip_gram.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1teabEFwD99J4OpzqCVf7GE96NFJ3W4Ew
"""

!ls
import pandas as pd
import numpy as np
import nltk
nltk.download('popular')
from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize 
from nltk import RegexpTokenizer
from nltk.stem import WordNetLemmatizer
from gensim.models import Word2Vec

sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')

df  = pd.read_csv("./content5M11.csv",encoding = "ISO-8859-1")

df



tokenizer = RegexpTokenizer(r"\w+")
new_col = []
for one_sentence in df['content']:
  if(type(one_sentence)==float): #可能有NA
    new_col.append('')
    continue
  new_col.append(tokenizer.tokenize(one_sentence))

df['tokenizer'] = new_col
df

#array['new_col']
#stop_words = set(stopwords.words('english')) 
#word_tokens = word_tokenize(text) 
#filtered_sentence = [] 
#for w in word_tokens: 
#    if w not in stop_words: 
#        filtered_sentence.append(w)

import numpy as np
text_size = 500
#text_total = np.zeros(text_size)

myWord2 = Word2Vec(df['tokenizer'],size=text_size,iter=10,sg=1,min_count=1)

print(myWord2)

print(myWord2.similar_by_word('Trump')[0])

a_array = myWord2.wv.get_vector('Trump')
pd.DataFrame(a_array).to_csv("./file.csv")

df2 = pd.read_csv("./openie_data.csv",encoding = "ISO-8859-1")
df3 = pd.read_csv("./openie_data.csv",encoding = "ISO-8859-1")

df2[1090:1092]

sub_vector = df2['sub']
relation_vector = df2['relation']
obj_vector = df2['obj']
embed_sub = df2['sub']  ####
embed_relation = df2['sub']  ####
embed_obj = df2['sub']  ####
time = df2['time']
stock = df2['stock']

type(sub_vector)

# embeded_sub
for i in range(len(sub_vector)):
  array_token = tokenizer.tokenize(sub_vector[i])
  text_total = np.zeros(text_size)
  for text in array_token:
    text_total += myWord2.wv.get_vector(text)
  embed_sub[i] = text_total / len(array_token)

embed_sub

# embeded_relation
for i in range(len(relation_vector)):
  array_token = tokenizer.tokenize(relation_vector[i])
  text_total = np.zeros(text_size)
  for text in array_token:
    text_total += myWord2.wv.get_vector(text)
  embed_relation[i] = text_total / len(array_token)

embed_relation

# embed_obj
for i in range(len(obj_vector)):
  array_token = tokenizer.tokenize(obj_vector[i])
  text_total = np.zeros(text_size)
  for text in array_token:
    text_total += myWord2.wv.get_vector(text)
  embed_obj[i] = text_total / len(array_token)

embed_obj

import json

d2 = {'time':df2['time'],'stock':df2['stock'],'sub_vector':df3['sub'],'relation_vector':df3['relation'],'obj_vector':df3['obj'],'embed_sub': embed_sub, 'embed_relation': embed_relation, 'embed_obj': embed_obj}
datafram = pd.DataFrame(data=d2)
datafram
###  用不到  datafram.to_csv("./result.csv",encoding = "ISO-8859-1")

miss = datafram.sample(frac=1)
datafram1 = miss[0:1151]
datafram2 = miss[1152:2152]

#datafram.to_json()
class NumpyEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        return json.JSONEncoder.default(self, obj)
my_time = np.array(datafram1['time'])
my_time = json.dumps({'time': my_time}, cls=NumpyEncoder)

my_stock = np.array(datafram1['stock'])
my_stock = json.dumps({'stock': my_stock}, cls=NumpyEncoder)

my_sub_vector = np.array(datafram1['sub_vector'])
my_sub_vector = json.dumps({'sub_vector': my_sub_vector}, cls=NumpyEncoder)

my_relation_vector = np.array(datafram1['relation_vector'])
my_relation_vector = json.dumps({'relation_vector': my_relation_vector}, cls=NumpyEncoder)

my_obj_vector = np.array(datafram1['obj_vector'])
my_obj_vector = json.dumps({'obj_vector': my_obj_vector}, cls=NumpyEncoder)

my_embed_sub = np.array(datafram1['embed_sub'])
my_embed_sub = json.dumps({'embed_sub': my_embed_sub}, cls=NumpyEncoder)

my_embed_relation = np.array(datafram1['embed_relation'])
my_embed_relation = json.dumps({'embed_relation': my_embed_relation}, cls=NumpyEncoder)

my_embed_obj = np.array(datafram1['embed_obj'])
my_embed_obj = json.dumps({'embed_obj': my_embed_obj}, cls=NumpyEncoder)

json_file = [json.loads(my_time),json.loads(my_stock),json.loads(my_sub_vector),json.loads(my_relation_vector),json.loads(my_obj_vector),json.loads(my_embed_sub),json.loads(my_embed_relation),json.loads(my_embed_obj)]
with open('./data.json', 'w') as f:
    json.dump(json_file, f)
#json.loads(my_embed_obj)
#json.loads(my_embed_relation)
#json.loads(my_embed_sub)
#json.loads(my_obj_vector)
#json.loads(my_relation_vector)
#json.loads(my_sub_vector)

#datafram.to_json()
class NumpyEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        return json.JSONEncoder.default(self, obj)
my_time = np.array(datafram2['time'])
my_time = json.dumps({'time': my_time}, cls=NumpyEncoder)

my_stock = np.array(datafram2['stock'])
my_stock = json.dumps({'stock': my_stock}, cls=NumpyEncoder)

my_sub_vector = np.array(datafram2['sub_vector'])
my_sub_vector = json.dumps({'sub_vector': my_sub_vector}, cls=NumpyEncoder)

my_relation_vector = np.array(datafram2['relation_vector'])
my_relation_vector = json.dumps({'relation_vector': my_relation_vector}, cls=NumpyEncoder)

my_obj_vector = np.array(datafram2['obj_vector'])
my_obj_vector = json.dumps({'obj_vector': my_obj_vector}, cls=NumpyEncoder)

my_embed_sub = np.array(datafram2['embed_sub'])
my_embed_sub = json.dumps({'embed_sub': my_embed_sub}, cls=NumpyEncoder)

my_embed_relation = np.array(datafram2['embed_relation'])
my_embed_relation = json.dumps({'embed_relation': my_embed_relation}, cls=NumpyEncoder)

my_embed_obj = np.array(datafram2['embed_obj'])
my_embed_obj = json.dumps({'embed_obj': my_embed_obj}, cls=NumpyEncoder)

json_file = [json.loads(my_time),json.loads(my_stock),json.loads(my_sub_vector),json.loads(my_relation_vector),json.loads(my_obj_vector),json.loads(my_embed_sub),json.loads(my_embed_relation),json.loads(my_embed_obj)]
with open('./data2.json', 'w') as f:
    json.dump(json_file, f)

import requests
import json

r = requests.get('http://120.126.19.107:3000/time_test')
print(r.status_code)  # 200成功
array = json.loads(r.content)  # str 轉乘json
print(len(array))

r = requests.get('http://120.126.19.107:3000/time_test')
print(r.status_code)  # 200成功
array = json.loads(r.content)  # str 轉乘json
array.sort()
print(array)